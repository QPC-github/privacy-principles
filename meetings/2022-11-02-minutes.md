Users
dm
dmarti
ü¶â
Guest


1
# Privacy TF - Wed 2 Nov 2022
2
‚Äã
3
Present: Dan, Amy, Jeffrey, Christine, Sam, Pete, Robin, Don, Nick, Wendy
4
‚Äã
5
## [PR 191](https://github.com/w3ctag/privacy-principles/pull/191)
6
‚Äã
7
Nick: a variety of comments on "deception" - this is an alternative
8
‚Äã
9
Peter: is it intended to say UAs should not support deception?
10
‚Äã
11
Nick: No, it still says they should, but it should be in rare cases...
12
‚Äã
13
[we look at the PR]
14
‚Äã
15
Pete: "incorrect information" sounds like an error... something like...
16
‚Äã
17
Jeffrey: inaccurate?
18
‚Äã
19
Dan: eg. geolocation. The device sensors say a certain thing. That doesn't mean it's necessarily the truth. It just means the devices subsystem that plugs into the sensors and other things and comes up with a location has come up with this. But the user wants to say this. It's not necsesarily deception, and it's not inaccurate, and it's not incorrect. it's just at variance with what the underlying hardware says. Same with ambient light or temperature or whatever. That seems to me.. that's why I bristle at any of these words. 
20
‚Äã
21
Nick: APIs should be designed so it doesn't assert a fact - ...
22
‚Äã
23
Pete: I like that. I think there's just some consistency. But that is very appealing.
24
‚Äã
25
Don: the user's location can be the location they're affiliated with, the location where the http client software is even though they're on remote desktop, and the location where the display is. If you're a liverpool fan in san francisco using a remote desktop from a data center in kansas city the only location that's relevent is the location the user chooses to represent
26
‚Äã
27
Jeffrey: when we write these apis that by default return particular sensor values from the device, and geolocation is one that's often wrong, but a camera or a screenshare api, those are by default going to return a particular thing from the device. Websites are likely to expect that is those sensor values. Saying the API should be designed not to assert that this is true, doesnt' really change what is generally going to happen. it's usually going to be a particular value. The threat model argument is good. Should not assume it is that sensor value because users can send something else. But I'm not sure that it helps to debate how we word that this sensor value migth not be returned by this api when it usually is.
28
‚Äã
29
Dan: does that change anything about the wording of the PR..?
30
‚Äã
31
Jeffrey: I'm arguing against nitpicking in order to get something that doesn't reflect what the apis actually do. Fine with other tweaks, comments trying to make things shorter and more straightforward. Comment about identity is already a different principle - cite or remove from here. 
32
‚Äã
33
Nick: I'm not sure we should say 'need to'..
34
‚Äã
35
Jeffrey: the users may still configure their agents is a statement of fact - a claim about those APIs. Instead we should be saying something normative. Maybe the APIs should still allow users to configure their agents. Statement 'may' may not be true if the api is designed badly or the implementation is bad.
36
‚Äã
Privacy TF - Wed 2 Nov 2022
Present: Dan, Amy, Jeffrey, Christine, Sam, Pete, Robin, Don, Nick, Wendy

PR 191
Nick: a variety of comments on "deception" - this is an alternative

Peter: is it intended to say UAs should not support deception?

Nick: No, it still says they should, but it should be in rare cases...

[we look at the PR]

Pete: "incorrect information" sounds like an error... something like...

Jeffrey: inaccurate?

Dan: eg. geolocation. The device sensors say a certain thing. That doesn't mean it's necessarily the truth. It just means the devices subsystem that plugs into the sensors and other things and comes up with a location has come up with this. But the user wants to say this. It's not necsesarily deception, and it's not inaccurate, and it's not incorrect. it's just at variance with what the underlying hardware says. Same with ambient light or temperature or whatever. That seems to me.. that's why I bristle at any of these words.

Nick: APIs should be designed so it doesn't assert a fact - ...

Pete: I like that. I think there's just some consistency. But that is very appealing.

Don: the user's location can be the location they're affiliated with, the location where the http client software is even though they're on remote desktop, and the location where the display is. If you're a liverpool fan in san francisco using a remote desktop from a data center in kansas city the only location that's relevent is the location the user chooses to represent

Jeffrey: when we write these apis that by default return particular sensor values from the device, and geolocation is one that's often wrong, but a camera or a screenshare api, those are by default going to return a particular thing from the device. Websites are likely to expect that is those sensor values. Saying the API should be designed not to assert that this is true, doesnt' really change what is generally going to happen. it's usually going to be a particular value. The threat model argument is good. Should not assume it is that sensor value because users can send something else. But I'm not sure that it helps to debate how we word that this sensor value migth not be returned by this api when it usually is.

Dan: does that change anything about the wording of the PR..?

Jeffrey: I'm arguing against nitpicking in order to get something that doesn't reflect what the apis actually do. Fine with other tweaks, comments trying to make things shorter and more straightforward. Comment about identity is already a different principle - cite or remove from here.

Nick: I'm not sure we should say 'need to'..

Jeffrey: the users may still configure their agents is a statement of fact - a claim about those APIs. Instead we should be saying something normative. Maybe the APIs should still allow users to configure their agents. Statement 'may' may not be true if the api is designed badly or the implementation is bad.

Dan: your wording is more in line with a user need?

Jeffrey: yeah and advice to api authors

Nick: I intended it to be a statement of fact, like you should prepare for this because it sgoing to happen

Jeffrey: advice to the website?

Nick: or i'd say.. "need to" suggests users are allowed but only if they need that

Jeffrey: not what I want to imply. If you're addressing the site you need something before this paragraph saying sites should.. before users should configure their agents, it's clear it's advice to sites and not a statement about apis. Reversing these paragraphs needs someone to read over it. Happy to merge if Nick wants to do a pass to make sure it reads well and then merge it that's fine with me.

Don: sensor can be wrong and user is right

Pete: not a blocking request, but i suggest just using the term "arbitrary" instead of "incorrect" or anything that suggests "correctness"

Robin & Amy & Sam: +1

Dan: +1 to merge

Nick: if we agree some changes but with the general approach I'd be happy to hand it over to editors to merge and edit.

Jeffrey: sure

Dan: that means we're not merging the previous deception PR and closing it

Nick: Pete if you have more comments can you add them?

Pete: I can do this in the next 24 hours. As part of this PR?

Jeffrey: I can wait for a day, then merge.

Vulnerable People
Proposal from Christine:

**Vulnerability**: Some individuals may be more vulnerable to privacy risks or harm
as a result of collection, misuse, loss or theft of personal data because of their
attributes, interests, opinions or behaviour. Others may be vulnerable because of
the situation or setting (e.g., where there is information asymmetry or other power
imbalances) or because choices are not presented in an easy-to-understand
meaningfully way (e.g., dark patterns). Yet others may be vulnerable because they
have not been consulted about their privacy needs and expectations or included in
the design of the product of service. Sometimes communities of individuals are
classed as ‚Äúvulnerable‚Äù, typically children and the elderly, but anyone could be
privacy vulnerable in a given context.

**Children**: All individuals, whether children or not, face a broad spectrum of privacy
risks on the Internet. However, children are often regarded as particularly
vulnerable because they may lack awareness of the potential consequences of privacy
risks and have limited ability to assess risk and make decisions. Nonetheless,
children have their own right to privacy and should be encouraged to exercise that
right. No one size fits all approach for dealing with child privacy. A population of
children is not homogeneous. Each child is different: age, education, language,
culture, maturity, experiences, interests, and privacy awareness. Parents, guardians,
and peers can help children make their own decisions about privacy, by raising
awareness, teaching risk assessment, setting a good example and modifying their level
of involvement as children mature. Children must also have unencumbered access to
privacy to be able to report abuse, for self-determination, and to seek help., among
other things.
Christine: I like the comment from Don that at the time you make choices you don't know if you will become 'vulnerable'. Should I put this in a PR?

Robin: I like the paragraph, and Don's note about the timeshifting of vulnerability. Less sure about adding a specific paragraph about children.

Christine: I'd be happy to take it out.

Robin: as would I

Jeffrey: I'm nervous about leaving children out because parents often assume they own their children and it would be nice to have something challenging that assertion, but it does make the document more controversial

Robin: agree it's a problem, but we're trying to get the document away from specific issues and stay more in the principle area.. there's a whole other bunch of issues with vulnerable people that also should be considered. I'd be worried about calling this out specifically

Dan: agree with a lot of the text that's in here, but probably better not to have it. Instead of getting rid of it, maybe we can bring the essence of it..

Jeffrey: something about guardians - guardians responsible for protecting privacy of a person, but also people need to be able to escape an abusive guardian. Britney Spears as and example of someone with an abusive guardian. The tension between the protector.. who watches the watchers.. is worth having somehow.

Nick: I would like us to add.. I think the important point we're making is that children need some privacy and autonomy even from the people who are helping them, and we want to make that for all vulnerable populations. Let's make that point for everyone, and include children as an example

Robin: Not just abusive, but also overbearing guardians

Christine: can do the first pass. Tried to write in a balanced and careful way to recognise both interests of parents and children.

PR: 190 fake consent
Don: We discussed last time.. turns out that at PATCG people are talking about a fairly awkward combination of regulation and processing within the browser. A little background is the proponants of privacy preserving attribution systems that would somehow give you information about the statistical properties of an ad campaign without revelaing information about the idnividuals inside it, people proposing that wnat to do a system that ethically could be turned on within the browser without consent. And that is pushing the limit of the philosphy and the math. But at the same time we've got real world legal requirements that say this is a type of processing for which consent must be obtained. So the way that things are headed right now, which may turn out to be a common pattern, is that processing for this type of systems can be something that according tot he principles does not require consent, but according to the law does require consent. Under the law though you don't need to get full Robin-compliant consent. You just need to get people to click a button to make this dialog go away. Fake consent is harmful privacy labour. What i want to be able to say is even if you don't need real consent don't try for fake consent. This took me a surprising amount of time to try to express and I don't know how well I did.

Jeffrey: Continue to explore the example from the PATCG, sounds like getting real consent for the privacy preserving system that they want to devleop would be really hard becuase it's hard to explain what's going on. But getting informed consent for a simpler system that revelas more information would be easier. so our advice might be design a system that invades the users privacy more becuase it's easier to explain. I don't like that advice.

Don: I think there's a second order set of effects of the existence of these privacy preserving system that makes that not good adivce.

Christine: I dislike this term 'fake' consent, but understand that it's a thing. Is it really what you're saying.. is companies should not be doing things to give the illusion that they have permission to do it? The problem you're citing seems to me if the law is the problem and it's not practical to obtian consent and it's a case where consent really isn't necessary, there is a case to be made to go to the peopl ewho make the laws and say here's an example of why we shouldn't need consent, get permission that way.. I don't think we should .. something about this troubles me

Jeffrey: I definitely agree with the being troubled aspect. The key law here is the eprivacy directive that says you have to get consent in order to sotre anything on the users computer. That is a bad privacy poliyc. Storing things is not necssarily a privacy problem that requires consent. But the law requires you to get consent, but also allows the consent to be meaningless. Not informed. SO the practice is that when the law is requiring soething silly, do the minimum the law reuires, even if it's not what you would do in order to get consent for something that actually needs consent. I'm not sure I disagree with that practice. Don's principle says that practice is bad, but I think I like that practice when it's a bad law.

Don: we've got a better understanding of what consent really is since these laws came out. it's possible that some of the academic research on consent in other contexts was done before that eprivacy directive came out, but we really only got to understand internet consnet now we've seen a bunch of examples of things that the industry calls consent that aren't actually consent

Robin: my concern is what do we .. are we expecting to prescribe? If i'm in the shoes of someone making a website, and I know this is safe, my privacy team as reviewed it, it has been designed by the best mind in privacy but there's this law from 1995 based on consent grounded in unrealistic things that's requiring me to dance with a frog on my head, should I not dance with a frog on my head, or can I take a veyr little frog and tap my foot. What you gonna do? I don't want to do this, but having been in those shoes, you try to make it so it's as unpainful to your users as possible ot comply with a useless legal requirement. I don't disagree with you that people shouldn't be designing systems [...] consent, but what's the principle here for a website or a browser? How can you deal with it, except campaign for replacement privacy legislation?

Nick: I'm concerned about blaming the laws. There are many cases where some websites have said we have to do this because of this law and those people are lying. They're not actually following the law. they're just harassing usres and saying they had no choice. We shouldn't say either it was okay that you bother users in that way, or that you were following the law.

Don: There's actual compliance with the law, and then there's what people can get away with. No delivery serivce that could make a profit if they followed every traffic and parking law all the time. They're not compliant with the law but they're behaving within the envelope of safety both within and just outside the law.

Dan: when I initially read the text I was thinking about companies that are making a show of privacy consent, to make some kind of marketing point, we care about your privacy, please agree to these 17 things, but it's not meaningful. That seems an antipattern which sometimes you do see. Is that worth calling out?

Jeffrey: different direction.. I see Nick's piont that it's not great to say some laws are bad so you should do the minimum to follow them. Many of the laws in this area are good and following them leads companies to improve their stuff. But also I don't think we can say dont' do fake consent when there is this silly law that requires consent in places where it doesn't make sense to use the user's attention to really inform them. Maybe the best thing is to not address this topic, and to write our principles about when you get consent, and let companies deal with the fact that their compliance team maybe makes them get consent when they don't need to, and not address fake consent in our doc

Nick: I'd prefer that. You could have a law that reuqires something that's not consent and doens't have privacy impact, and it's just a bad law and not a privacy law. We don't have to talk about non privacy laws. I think we should try to say something about don't put unnecessary privacy labour that only purports to gather consent.

Robin: is there a mniimise privacy labour giving existing constraints principle we could crank out from this? Figure out how it applies..

Don: Problem is you could minimise privacy labour by getting the broadest possible fake consent as early as possible.

Jeffrey: we have other principles saying get real consent. A site that's trying in good faith to do what we're recommending and balance the principles and think like we want them to. If theyr'e trying to cheat they're going to cheat.

Dan: we do have wording about not burdening users with additional privacy labour. I think it does make sense to hav ea principle which makes that point. It makes sense to bring it into a principle about privacy labour that could also address the point don mentioned that in reducing privacy labour you should not make consent more meaningless somehow. That's a very nuanced statement.

Don: the problem is burying the user in fake consent requests because fake consent is good enough to get by for certain uses of peoples data. That makes it harder for people to recognise and consider a real consent request when they have one.

Jeffrey: I don't think that's true. I think a fake consent request is worded in a very different way from an informed consent request. There's going to be mroe text informing the users. I expect it's easy for a user to tell them apart.

Don: fake ones will mimic real ones. The fake ones will camoflage themselves with verbiage

Jeffrey: true for bad faith sites, but not for sites trying to do the right thing and also present a consent reuqests becuase the law requires them to and not because it's a privacy benefit

Don: sites are a/b testing what gets them the most revenue. Something that could seem bad faith in the end result is the result of iterative UX testing. THe mimicing of a real consent request by a fake consent request would not necessarily be done by a bad faith site, but just the evolutionary outcome of repeated a/b tests.

Dan: is there an issue on this topic? Maybe we need to bring this back to discussion in an issue. I'd like to suggest this is not for v1 of this document given we need longer to reach consent.

Don: I'll make an issue

PR #182
Jeffrey: will try for next week, taking Pete's suggestion

PR 170
Dan: marked as overtaken and consider closing

Nick: +1

PR 186
Nick: opened last week

Jeffrey: ready to merge

Dan: looks good to me

Nick: conflicts.. will fix

Issues
harassment
Wendy: straying a bit, and not sure we need it in v1

Dan: backburner

Robin: backburner and I'll remove the empty section

Nick: I will write something, I strongly do not want it removed.

aggregate..
Sam: what do we think..?

Nick: hoping we can close that with text we've merged on ancillary, or if not very soon.

Dan: which PR?

Nick: 184

Dan: [updates issue]

Identity on the web
Pete: deception text addresses this? I can check with issue opener that it addresses concerns.

partitian definition and storage
Dan: anyone to write?

Wendy: I'll take it

