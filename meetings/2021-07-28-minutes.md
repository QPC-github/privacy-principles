# Privacy Taskforce Call - 28 July 2021

Present: Dan, Amy, Robin, Jeffrey, Tess, Pete

Regrets: Christine

## Priorities / moving forward

Robin: after general editorial & making coherant, go section by section and see if we agree eg. with definitions etc.

Dan: create issue for each review section and structure agenda with them?

## [Contextual Integrity](https://github.com/w3ctag/privacy-principles/issues/22)

Jeffrey: had a discusion internally in google about contextual integrity.. we might end up with weaker constraints than this group would like. Each context has norms around how you're supposed to use that information - privacy is those norms are followed. The concern is that google and surveillence tech have set the norms for information sharing on the internet so people expect mor ethan they're happy with so it ends up being the norm and we can't constrain it well

tess: if we leave it as open as status quo we'd be failing

Robin: fair concern. Always been a week spot of contextual integrity, the value is how it breaks down a context and gives you a framework to detail specifics of context, if we intervene on thsi aspect it switches it from appropriate to inappropriate. It does kick the can down the road towards some decision around what is normatively privacy. I dont know how much better we can do than that. What's the alternative? decontextualised privacy? unlikely tow ork, exactly the problem that contextual integrity addresses. You can always find counter examples if you decontextualise. This breaks it down. I would hope, it's true there's been a normalising factor of the past ten years of surveillence but I'm not sure if that normalising factor has affected people as much as technologists. Most people have no clue what's going on. Whenever they get the slightest hint of what's happening they're horrified. Even technologists when you explain how data flows they are too.. some industries have greater unerstanding. People in marketing .. "we're not using it in a wrong way" .. not that much of a normalising factor

Dan: discussion is about how much we want to focus on the topic or build on the framework of contextual integrity? I'll always remember Alicia McDonald (?) presented research a while back.. that if you asked people who were web users but not familiar with how advertising works to describe how ads work they'd describe how advertising worked in the 90s. If you described how it actually worked they'd say it can't be possible that would be illegal.. That phrase rings in my head. Resonates with me that most people.. and recent data about how iOS started to ask for more permission and many people were not allowing, does indicate what we as technologists consider the norm is not actually what people would consider the norm, or that they want to live in.

Robin: resigned to it is a real effect - "I hate it but don't know what else I can do" - studies on this. But different from people thinking it's the right thing to do. People think they've lost control over their data and there's nothing they can do about it, but they're not saying it's great.

Dan: opened an issue on this topic

Jeffrey: it makes sense to say in this context here is what we think the norms are. That starts fixing concern my coworker had. Question came up at IETF meeting - we presented FLoC to the privacy research group - there are questions about how much data will be aggregated and shared with websites. How do we think that fits in with the norms that we think people have in browsing? What control do they need in order to make that fit with their norms and so on? To sketch the FLoC 2.0 plans, it's related to ad topics proposal from privacy CG - give people human understandable topics, guess the topics based on their browsing behaviour or let them configure it in the browser and leave the choice between those up to browsers. Then share some random subset of the user's topics with websites chosen weekly or so.

Dan: I don't know how far we want to go down in terms.. I don't want us to review FLoC in this workstream. Try to remain high level and find a conceptual framework that would help epeople reviewing and people designing FLoC to have a shared framework.

Jeffrey: in order to produce a conceptual framework.. I agree.. but we're going to have to look at some examples.

Robin: agree, not doing review for stuff, we shouldn't. But would be great to.. FLoC is an interesting use case with some valuable things, some problemantic things.. we should show our thing works with a real world case. Let's see how our things hold up. Can we inform the discussion? If not what we're doing is not that useful. Good contact with reality. Two things that might surface in using what we currently have to evaluate FLoC that might be problematic - one is issues of collective privacy. FLoC surfaces something understudied but true in privacy, that a lot of today's mechanisms will apply to even if I protect my privacy if I'm similar to someone who doesn't then my privacy is still being invaded because it can be guessed. The other is issues of what we call publisher sovreignty. Is it okay for another party to use behaviour on a publisher property for monetisation elsewhere without any control. Might be out of scope but the issue next door. I have no idea what to do with it in this context.

Dan: sounds related to a TAG issue we are looking at - the CHIPS proposal - in that discussion we started to talk about the nhs putting facebook like buttons on all of their pages 10 years ago and how this unintentionally leaked data about symptoms people were looking at, unintentionally by user and unintentionally by nhs. It was a scandal. Feels like another case we ought to be talking about. Is that the same as publisher sovreignty?

Robin: these things are intertwined. When people are being tracked across contexts publishers lose sovereignty over their audience knowledge. In the NHS case the NHS didn't specifically want to maintain control over its audience, not monetising it independantly. 

Dan: but there was unintentional data leakage. Seems like that notion ought to be specified in this document. If you have one party and data is leaking.. as a patient going to NHS expecting to look up information, and the information is going to facebook without my knowledge, that's leaking across a privacy boundary

Robin: definitely yes. Difference is in your case NHS still have control. They might not have meant to share information with facebook, but the second they realise they remove the like buttons. Problem with FLoC is different - apart from blocking browsers that support FLoC from visiting our website we have zero control over whether that can be used. Might be limited to whether we run ads or not which if you have to run ads you have to run ads. More of a situation where how can we protect that audience data. Maybe the user has agreed to that. But it's taking data from the publisher and putting it somewhere else. Another layer of that issue.

Jeffrey: reminds me of sharing contact data. The consent to share an address book belongs to the person sharing it and the people in the address book. A single person doens't have that right. Even if the user has consented to FLoC, the publisher also needs to consent. Designe din but may not be adequate. In many cases the user doens't have the right to consent to give away information they have the ability to give away.

Robin: interesting concept. Worth writing down. One aspect I'm not sur ehow to take into account - financial pressure. You talk to a lot of small struggling publishers who would love to do privacy, but they can't. the notion of consent when it's etiher do it or go out of businesses is problematic. We can't solve everything, but..

Jeffrey: consent can't be coerced. Financial coercion is a form of coercion.

Robin: yes.

Tess: agree

Jeffrey: often technically infeasible to get multi party consent. more feasible for publisher-user relationships

Robin: for stats it really isn't

Jeffrey: we can say this is the goal.. here's what we're trying to do, not sure about the tradeoffs

Robin: we can say we're aware it's an issue.  Not new. Debating about medical research since the 50s. We can flag it as an issue and point to the fact that this is one of those things that can't necessarily be solved by tech. Governance models we can point to that societies can think of. A lot of the prohibitions in the gdpr stem from this. Collections of race or politics or union data are not about one person, all about doing stats you shouldn't be doing

Dan: gdpr concept of sensitive categories?

Robin: yes, article 9. Also something we might want to look at. It's good it's there but it's a blunt instrument. We have to be careful. We keep running into the issue of allowing people to specify own gender.. of course.. but once you do that you start to have gender information that might run afoul of article 9 limitations. If we don't do it's bad, if we do it it's bad..

Dan: there is an argument to.. I remember from working for UK GDS there was good guidence that GDS issued which is don't collect gender unless you need gender for some reason. There are so many forms that ask for gender.. why?

Robin: in english there's no good reason. Different in heavily gendered languages.

Tess: even in english, ask for pronoun not the gender. If the reason is to know how to refer to someone, that's what you should ask them.

Dan: but don't ask unless you need it for some reason is related to concept of [data minimisation](https://www.w3.org/2001/tag/doc/APIMinimization.html). Should be part of this document as well maybe. That seems to me to be a fundamental privacy principle. Don't have information that you don't need. DOn't transmit information somebody didn't ask for.

Robin: data minimisation can get hooked into this by looking at purpose.. you can't start analysing a privacy statement if you don't know the purpose of data collection. if you're collecting something without a puprpose you shouldn't be collecting it  in the first place. That gives you data minimisation popping out. 

Jeffrey: worth noting in an issue the question of multi party consent. Anything else as a separate issue?

Robin: data minimisation?

Pete: sounds like a conversation we'll have later, also christine's concern. And what to do about https://github.com/w3ctag/privacy-principles/issues/7?

Robin: [opened issue: https://github.com/w3ctag/privacy-principles/issues/23]


Jeffrey: Re 7, describe in the issue, but not in the main document, for now


Dan: Anything else for today or to tee up for future weeks? Will file issues for reviewing each section.



## [There is no "latest published version"](https://github.com/w3ctag/privacy-principles/issues/16)

## [Follow the TAG Style Guide](https://github.com/w3ctag/privacy-principles/issues/15)

## [Describe paradigms for data use](https://github.com/w3ctag/privacy-principles/issues/11)

## [related docs](https://github.com/w3ctag/privacy-principles/issues/9)

## [Address device owners](https://github.com/w3ctag/privacy-principles/issues/2)

Jeffrey: need to write text

## [Figure out the default privacy boundary for the web](https://github.com/w3ctag/privacy-principles/issues/1)

Pete: concerned about things that are or aren't privacy boundaries.. want Christine's opinion

Jeffrey: we dno't have to constrain ourselves to only privacy boundaries. Need to schedule a call on this topic.

Dan: if there's anything that worries Tess in last week's discussion we should discuss further

Tess: I'll look

## [Remove noexport attributes left over from Bikeshed import.](https://github.com/w3ctag/privacy-principles/pull/19)

Jeffrey: editorial, merge-able

## [Add Makefile rules for building a snapshot with respec on the command line.](https://github.com/w3ctag/privacy-principles/pull/18)

Tess: Does it work outside of Mac?

&lt;silence>

Jeffrey: Sounds like this will help you, and not hurt anyone else; let's merge it and fix any problems later.

Robin: Approves

Dan: &lt;Fights with the fact that the build is broken; forces merge.>

## [Update the Status of this Document to be more in line with other W3C documents.](https://github.com/w3ctag/privacy-principles/pull/17)

Tess: maybe shouldn't merge it, implies we can publish something we can't

Jeffrey: could call it something other than a draft finding

Tess: intent is to publish as a TAG finding

Dan: I don't think that precludes this tf publishing as a draft finding. We can. Only the TAG can approve as a finding. This is a TAG task force. TAG task forces can draft TAG findings.

Jeffrey: Think this is clear that we only "intend" things, and we won't be the ones publishing as a Finding.

Tess: "publication as a draft finding does not imply endorsement.."

