# TAG privacy tf - 28 September 2022

Present: dan, don, nick, wendy, christine, pete, jeffrey, jonathan
Regrets: robin, amy


new apis and justification based on old ones: Jeffrey: no updates on that this week yet.

deception cannot be discussed without robin

177/178 delayed since amy isn't here

170: 

Nick: I split the section as suggested and tried to describe telemetry and analytics as a cluster of uses for choices. and also noted the fingerprinting implications of both data in telemetry and revealing a user preference about telemetry

Pete: 

**... splitting up 170 into a few different PRs:**

https://github.com/w3ctag/privacy-principles/pull/183

Adds section 2.2 Data minimization... 

Don: people can take action on it

Jeffrey: I love this.

Dan: I like it.

Jeffrey: we should cite data minimization ... 

Pete: it was noted that it was not a finding...

Jeffrey: we can cite it as a web page.

Christine: the way I read the text it seems to be applying mimization to person data. Do we want a larger scope statement. There's work in IETF to reduce metadata and noise. I'm wondering whther we should make the principle a bit broader.  Data that's not clearly personal data... but could pose a risk.

Dan: i think it is out of scope for the taskforce... but could be part of a revamped finding.  We can look at it more broadly in reference to the IETF work you mentioned.

Nick: I think it's broad enough...  We define personal data broadly as well...
https://pr-preview.s3.amazonaws.com/w3ctag/privacy-principles/pull/183.html#dfn-data

Christine: it's a good def of personal data but ... recoginize this may be oiut of scope for the doc.

**consensus to merge 183**

https://github.com/w3ctag/privacy-principles/pull/184

Nick: around the idea of having a cluster of choices... writing a new spec that has telemetry or analytics ... should be a flag - so it's easier to have a clustered choice when users want to make a decision about telemetry & analytics...  and maybe we could get to the point where we should say to spec authors: this is how you can be sure this is just for telemetry and this is how you should define that flag.

Pete: I can re-reaise this to privacy CG... 
Nick: hope browsers can use it even without standardization at privacycg, but if privacycg wants to discuss then great

dan: editorially, principles should be moved higher.

dan: I like this. needs dialog with spec authors

jeffrey: this comes out of discussion with webperf. this has gone in circles for a long time, but this might help. not exactly clear what browsers will do with it. sense within chrome that this mode might not be used, or only used for new features, might not want to present users with an option to turn off this kind of functionality.
nick: always the case that browsers might not take advantage of it.

*discussion on possible Privacy CG work*

dan: this is the kind of discussion we need to have. TAG is getting more review requests about telemetry-type APIs. good to have it grouped, even if it won't always be used.

Pete: if this becomes part of ongoing conversation - reviews - labeling telemetry - this principle implies that specs need to define what the spec'ed functionality does when the user doeesn't want to share the info.

Don: point came up at TPAC - running personal info through a privacy-preserving system would give you a user tracking system that could be made default: on. So when talking about the type of thing users would choose to turn on if they understood it. I don't have confidence that these systems would pass that test.  I like the draft that we have better than the direction of patcg discussions of privacy preserving attributution.

*discussion on the need for user research*

Nick: "if the browser reveals"... I think generally the APIs should just be null when the functionality is turned off.

Dan: can't be prescriptive, but API designers should on case by case basis make it more difficult to reveal whether users have turned off telemetry or turned on a private browsing mode

Jeffrey: on pete's comment - what to do when the user decides not to send ... some browsers may decide based on user research to make the decision that users generally would decide to send ... on hard to tell if users turn off: we can try but it's likely to be difficult.

Dan: suggest to land this as is and iterate.

**consensus to land this PR as is and then iterate**

Nick: do we want to have a more general principle about not revaling private modes, not revealing... maybe larger?

Dan: we have text in the design principles on private modes and assistive tech so probably we don't need more text here.

https://github.com/w3ctag/privacy-principles/pull/177

Specifically: https://github.com/w3ctag/privacy-principles/pull/177#pullrequestreview-1116206748

Pete: on same site recognition... don't think it's correct.

Jeffrey: you go to a site you log in you close your browser you go back to the site you're still logged in.... That's benign but it's still same site recognition.  

Nick: same site recognition across clearing data...   I think that's an unintuitive use of cross-partition... 

Christine: Aren't we also concerned if they are recognised even if "not logged in"?

Jeffrey: yes though sites also save preferences... might not be a full identity but... still recognized.

Christine: I want to be careful that we recognize that...

Dan: distinction between "logging out" which is a functional use case and clearing site data.

Jeffrey: right. browser doesn't know when you've clicked log out.. but does know when you clear data.

Nick: is this trying .. in the *fingerprinting mitigation* guidance we're trying to list the types of things that could be used as a fingerprint. Is this identity section re-creating.  

